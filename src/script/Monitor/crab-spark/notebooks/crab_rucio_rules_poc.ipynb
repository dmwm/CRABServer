{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9af689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_user,\n",
    "    col, collect_list, concat_ws, greatest, lit, lower, when,\n",
    "    avg as _avg,\n",
    "    count as _count,\n",
    "    hex as _hex,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91309756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import libs from current directory, fallback to $PWD/../workdir if not found\n",
    "try:\n",
    "    from crabspark_utils import get_candidate_files, send_os, send_os_parallel\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{os.getcwd()}/../workdir')\n",
    "    from crabspark_utils import get_candidate_files, send_os, send_os_parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22946659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('crab-taskdb')\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9013878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear any cache left, for working with notebook\n",
    "# it safe to run everytime cronjob start\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c19eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secret path, also check if file exists\n",
    "secretpath = os.environ.get('OPENSEARCH_SECRET_PATH', f'{os.getcwd()}/../workdir/secret_opensearch.txt')\n",
    "if not os.path.isfile(secretpath): \n",
    "    raise Exception(f'OS secrets file {secretpath} does not exists')\n",
    "# if PROD, index prefix will be `crab-*`, otherwise `crab-test-*`\n",
    "PROD = os.environ.get('PROD', 'false').lower() in ('true', '1', 't')\n",
    "# FROM_DATE, in strptime(\"%Y-%m-%d\")\n",
    "START = os.environ.get('START_DATE', None) \n",
    "END = os.environ.get('END_DATE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For run playbook manually, set start/end date here\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2024-10-01\"\n",
    "# if cronjob, replace constant with value from env\n",
    "if START and END:\n",
    "    START_DATE = START\n",
    "    END_DATE = END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ed53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index name\n",
    "index_name = 'taskdb'\n",
    "# use prod index pattern if this execution is for production\n",
    "if PROD:\n",
    "    index_name = f'crab-prod-{index_name}'\n",
    "else:\n",
    "    index_name = f'crab-test-{index_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object\n",
    "start_datetime = datetime.strptime(START_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "end_datetime = datetime.strptime(END_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "# sanity check\n",
    "if end_datetime < start_datetime: \n",
    "    raise Exception(f\"end date ({END_DATE}) is less than start date ({START_DATE})\")\n",
    "start_epochmilis = int(start_datetime.timestamp()) * 1000\n",
    "end_epochmilis = int(end_datetime.timestamp()) * 1000\n",
    "yesterday_epoch = int((end_datetime-timedelta(days=1)).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404c437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "print(START_DATE, \n",
    "      END_DATE, \n",
    "      index_name,\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85c2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code block and following block is copied from Panos's script.\n",
    "# https://gitlab.cern.ch/cmsdmops/cmsdmops/-/blob/8da699db49097d7a58440e6058f022c3f93992e2/monitoring/kubernetes/src/rucio_activity_account_usage.py\n",
    "# see more in https://github.com/dmwm/CRABServer/issues/7798#issuecomment-2389265249\n",
    "def get_df_rses(spark):\n",
    "    \"\"\"Get Spark dataframe of RSES\n",
    "    \"\"\"\n",
    "    hdfs_rses_path = '/project/awg/cms/rucio/{}/rses/part*.avro'.format(datetime.today().strftime('%Y-%m-%d'))\n",
    "    df_rses = spark.read.format(\"avro\").load(hdfs_rses_path) \\\n",
    "        .filter(col('DELETED_AT').isNull()) \\\n",
    "        .withColumn('rse_id', lower(_hex(col('ID')))) \\\n",
    "        .withColumn('rse_tier', _split(col('RSE'), '_').getItem(0)) \\\n",
    "        .withColumn('rse_country', _split(col('RSE'), '_').getItem(1)) \\\n",
    "        .withColumn('rse_kind',\n",
    "                    when((col(\"rse\").endswith('Temp') | col(\"rse\").endswith('temp') | col(\"rse\").endswith('TEMP')),\n",
    "                         'temp')\n",
    "                    .when((col(\"rse\").endswith('Test') | col(\"rse\").endswith('test') | col(\"rse\").endswith('TEST')),\n",
    "                          'test')\n",
    "                    .otherwise('prod')\n",
    "                    ) \\\n",
    "        .select(['rse_id', 'RSE', 'RSE_TYPE', 'rse_tier', 'rse_country', 'rse_kind'])\n",
    "    return df_rses\n",
    "def get_df_locks(spark):\n",
    "    \"\"\"Get Spark dataframe of Locks\n",
    "    \"\"\"\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    locks_path = f'/project/awg/cms/rucio/{today}/locks/part*.avro'\n",
    "    locks = spark.read.format('avro').load(locks_path) \\\n",
    "                .filter(col('SCOPE') == 'cms') \\\n",
    "                .filter(col('STATE').isin(['O', 'R'])) \\\n",
    "                .withColumn('rse_id', lower(_hex(col('RSE_ID')))) \\\n",
    "                .withColumnRenamed('NAME', 'f_name') \\\n",
    "                .withColumnRenamed('ACCOUNT', 'account_name') \\\n",
    "                .withColumnRenamed('BYTES', 'f_size') \\\n",
    "                .withColumn('r_id', lower(_hex(col('RULE_ID')))) \\\n",
    "                .select(['rse_id', 'f_name', 'f_size', 'r_id', 'account_name'])\n",
    "    return locks\n",
    "def get_df_accounts(spark):\n",
    "    \"\"\"Get Spark dataframe of Accounts\n",
    "    \"\"\"\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    hdfs_rucio_accounts = f'/project/awg/cms/rucio/{today}/accounts/part*.avro'\n",
    "    df_accounts = spark.read.format(\"avro\").load(hdfs_rucio_accounts) \\\n",
    "        .filter(col('DELETED_AT').isNull()) \\\n",
    "        .withColumnRenamed('ACCOUNT', 'account_name') \\\n",
    "        .withColumnRenamed('ACCOUNT_TYPE', 'account_type') \\\n",
    "        .select(['account_name', 'account_type'])\n",
    "    return df_accounts\n",
    "def get_df_rules(spark):\n",
    "    \"\"\"Get Spark dataframe of rules\n",
    "    \"\"\"\n",
    "    hdfs_rules_path = '/project/awg/cms/rucio/{}/rules/part*.avro'.format(datetime.today().strftime('%Y-%m-%d'))\n",
    "    return spark.read.format('avro').load(hdfs_rules_path) \\\n",
    "        .filter(col('SCOPE') == 'cms') \\\n",
    "        .withColumnRenamed('name', 'r_name') \\\n",
    "        .withColumn('r_id', lower(_hex(col('ID')))) \\\n",
    "        .withColumn('s_id', lower(_hex(col('SUBSCRIPTION_ID')))) \\\n",
    "        .withColumnRenamed('ACTIVITY', 'activity') \\\n",
    "        .withColumnRenamed('STATE', 'rule_state') \\\n",
    "        .withColumnRenamed('RSE_EXPRESSION', 'rse_expression') \\\n",
    "        .select(['r_name','r_id', 's_id', 'activity', 'rule_state', 'rse_expression']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271b1c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add data_tier field\n",
    "df_rses = get_df_rses(spark)\n",
    "df_locks = get_df_locks(spark)\n",
    "df_accounts = get_df_accounts(spark)\n",
    "df_rules = get_df_rules(spark)\n",
    "tb_denominator = 10 ** 12\n",
    "locks = df_locks.join(df_rses, ['rse_id'], how='left') \\\n",
    "        .filter(col('rse_kind') == 'prod') \\\n",
    "        .select(['f_name', 'f_size', 'RSE', 'rse_type', 'account_name', 'r_id']) \n",
    "\n",
    "locks_with_activity = (\n",
    "    locks.join(df_rules, ['r_id'], how='leftouter')\n",
    "         .select(['f_name', 'account_name', 'RSE', 'rse_type', 'f_size', 'activity', 'r_name'])\n",
    "         .withColumn('data_tier', regexp_extract('r_name', r'^\\/([\\w-]+)\\/([\\w-]+)\\/([\\w-]+)(#[\\w-]+)?', 3))\n",
    "         .select(['f_name', 'account_name', 'RSE', 'rse_type', 'f_size', 'activity', 'data_tier'])\n",
    ")\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# A File locked by the user for two activities is accounted to both activities\n",
    "# A File locked by two users for the same activity is accounted to both Users\n",
    "user_aggreagated = locks_with_activity \\\n",
    "        .select(['f_name', 'f_size', 'RSE', 'rse_type', 'account_name', 'activity', 'data_tier']) \\\n",
    "        .distinct() \\\n",
    "        .groupby(['RSE', 'rse_type', 'account_name', 'activity', 'data_tier']) \\\n",
    "        .agg(_round(_sum(col('f_size')) / tb_denominator, 5).alias('total_locked')) \\\n",
    "        .join(df_accounts, ['account_name'], how='left') \\\n",
    "        .withColumnRenamed('RSE', 'rse_name') \\\n",
    "        .withColumn('timestamp', lit(timestamp)) \\\n",
    "        .select(['total_locked', 'rse_name', 'rse_type', 'account_name', 'account_type', 'activity', 'data_tier', 'timestamp']) \\\n",
    "        .cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_aggreagated.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e98534",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_aggreagated.count()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.5.0"
    },
    {
     "name": "spark.executor.instances",
     "value": "20"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
