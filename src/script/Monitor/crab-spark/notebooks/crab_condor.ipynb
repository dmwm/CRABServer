{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed9b54a",
   "metadata": {},
   "source": [
    "# CRAB Spark condor job\n",
    "\n",
    "This join info between the condor job metrics and crab taskdb, to answer these questions:\n",
    "- How many jobs use ignorelocality?\n",
    "- What is wall clock time spent by each CMS data tier and each job type?\n",
    "- What is the success rate of the Analysis job type?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9af689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_user,\n",
    "    col, collect_list, concat_ws, greatest, lit, lower, when,\n",
    "    avg as _avg,\n",
    "    count as _count,\n",
    "    hex as _hex,\n",
    "    max as _max,\n",
    "    min as _min,\n",
    "    round as _round,\n",
    "    sum as _sum,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to import libs from current directory, fallback to $PWD/../workdir if not found\n",
    "try:\n",
    "    import osearch\n",
    "    from crabspark_utils import get_candidate_files\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{os.getcwd()}/../workdir')\n",
    "    import osearch\n",
    "    from crabspark_utils import get_candidate_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22946659",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('condor-job')\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear any cache left, for working with notebook\n",
    "# it safe to run everytime cronjob start\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c19eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secret path, also check if file exists\n",
    "secretpath = os.environ.get('OPENSEARCH_SECRET_PATH', f'{os.getcwd()}/../workdir/secret_opensearch.txt')\n",
    "if not os.path.isfile(secretpath): \n",
    "    raise Exception(f'OS secrets file {secretpath} does not exists')\n",
    "# if PROD, index prefix will be `crab-*`, otherwise `crab-test-*`\n",
    "PROD = os.environ.get('PROD', 'false').lower() in ('true', '1', 't')\n",
    "# FROM_DATE, in strptime(\"%Y-%m-%d\")\n",
    "START = os.environ.get('START_DATE', None) \n",
    "END = os.environ.get('END_DATE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For run playbook manually, set start/end date here\n",
    "START_DATE = \"2024-09-28\"\n",
    "END_DATE = \"2024-10-01\"\n",
    "# if cronjob, replace constant with value from env\n",
    "if START and END:\n",
    "    START_DATE = START\n",
    "    END_DATE = END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430146eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index name\n",
    "index_name = 'condor-taskdb'\n",
    "# use prod index pattern if this execution is for production\n",
    "if PROD:\n",
    "    index_name = f'crab-prod-{index_name}'\n",
    "else:\n",
    "    index_name = f'crab-test-{index_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object\n",
    "start_datetime = datetime.strptime(START_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "end_datetime = datetime.strptime(END_DATE, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "# sanity check\n",
    "if end_datetime < start_datetime: \n",
    "    raise Exception(f\"end date ({END_DATE}) is less than start date ({START_DATE})\")\n",
    "start_epochmilis = int(start_datetime.timestamp()) * 1000\n",
    "end_epochmilis = int(end_datetime.timestamp()) * 1000\n",
    "yesterday_epoch = int((end_datetime-timedelta(days=1)).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404c437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "print(START_DATE, \n",
    "      END_DATE, \n",
    "      index_name,\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15887f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading file 2 days before start date and 1 days after end date inclusive\n",
    "# sometime flume (condor log aggregator) process the metrics is delay for 2 days, sometime it has timestamp from the future.\n",
    "# so we do this to make sure we get all metrics from the date we want. (all of these suggested by CMSMONIT)\n",
    "# Note that we read all files, compact or not, even it has the same content, we will dedup it in the next step.\n",
    "_DEFAULT_HDFS_FOLDER = \"/project/monitoring/archive/condor/raw/metric\"\n",
    "candidate_files = get_candidate_files(start_datetime, end_datetime, spark=spark, base=_DEFAULT_HDFS_FOLDER, day_delta=2)\n",
    "\n",
    "# this is map json doc to spark schema\n",
    "read_schema = StructType(\n",
    "        [\n",
    "            StructField(\n",
    "                \"data\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"RecordTime\", LongType(), nullable=False),\n",
    "                        StructField(\"CMSPrimaryDataTier\", StringType(), nullable=True),\n",
    "                        StructField(\"Status\", StringType(), nullable=True),\n",
    "                        StructField(\"WallClockHr\", DoubleType(), nullable=True),\n",
    "                        StructField(\"CoreHr\", DoubleType(), nullable=True),\n",
    "                        StructField(\"CpuTimeHr\", DoubleType(), nullable=True),\n",
    "                        StructField(\"Type\", StringType(), nullable=True),\n",
    "                        StructField(\"CRAB_DataBlock\", StringType(), nullable=True),\n",
    "                        StructField(\"GlobalJobId\", StringType(), nullable=False),\n",
    "                        StructField(\"ExitCode\", LongType(), nullable=True),\n",
    "                        StructField(\"CRAB_Workflow\", StringType(), nullable=True),\n",
    "                        StructField(\"CommittedCoreHr\", StringType(), nullable=True),\n",
    "                        StructField(\"CommittedWallClockHr\", StringType(), nullable=True),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "   )\n",
    "print(\"===============================================\"\n",
    "      , \"Condor Matrix and CRAB Table\"\n",
    "      , \"===============================================\"\n",
    "      , \"File Directory:\", _DEFAULT_HDFS_FOLDER, candidate_files\n",
    "      , \"Work Directory:\", os.getcwd()\n",
    "      , \"===============================================\"\n",
    "      , \"===============================================\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bcb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "crab_username = spark.sql(\"\"\"SELECT current_user() AS user\"\"\").toPandas().to_dict('records')[0]['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515aefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only \"interested data\" from condor metrics and save into temporary area\n",
    "# need to do this because we do not have enough memory to compute all data at once.\n",
    "# (1 days is ok, 1 month got spark OOM)\n",
    "# \"interested data\" is\n",
    "# - selected column (see `read_schema` above)\n",
    "# - date range from START_DATE inclusive to END_DATE exclusive\n",
    "# - only status Complete and type analysis\n",
    "# job will got dedup by `.drop_duplicates([\"GlobalJobId\"])` in later step\n",
    "( \n",
    "    spark.read.option(\"basePath\", _DEFAULT_HDFS_FOLDER)\n",
    "         .json(\n",
    "            candidate_files,\n",
    "            schema=read_schema,\n",
    "         )\n",
    "         .select(\"data.*\")\n",
    "         .filter(\n",
    "            f\"\"\"Status IN ('Completed')\n",
    "            AND Type IN ('analysis')\n",
    "            AND RecordTime >= {start_epochmilis}\n",
    "            AND RecordTime < {end_epochmilis}\n",
    "            \"\"\"\n",
    "         )\n",
    "        .drop_duplicates([\"GlobalJobId\"])\n",
    "        .write.mode('overwrite').parquet(f\"/cms/users/{crab_username}/condor_vir_data\" ,compression='zstd') # overriding the same path to cleanup old data. However, we could not run it parallel\n",
    ")\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ac50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_df = spark.read.format('parquet').load(f\"/cms/users/{crab_username}/condor_vir_data\").cache()\n",
    "condor_df.createOrReplaceTempView(\"condor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_CRAB_part = f'/project/awg/cms/crab/tasks/{END_DATE}/' \n",
    "crab_df = spark.read.format('avro').load(HDFS_CRAB_part)\n",
    "# we did not filter the task here because most jobs was created from older tasks.\n",
    "# if there are too many crab tasks, it might be safe to filter out the tasks older than 30+7 days ago.\n",
    "crab_df = crab_df.select('TM_TASKNAME', 'TM_IGNORE_LOCALITY').cache()\n",
    "crab_df.createOrReplaceTempView(\"tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271b1c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query\n",
    "query = f\"\"\"\\\n",
    "WITH filter_tb AS (\n",
    "SELECT *\n",
    "FROM condor\n",
    "WHERE 1=1\n",
    "AND RecordTime >= {start_epochmilis}\n",
    "AND RecordTime < {end_epochmilis}\n",
    "),\n",
    "join_tb AS (\n",
    "SELECT RecordTime, CMSPrimaryDataTier, WallClockHr, CoreHr, CpuTimeHr, ExitCode, CRAB_DataBlock, TM_IGNORE_LOCALITY, GlobalJobId, CommittedCoreHr, CommittedWallClockHr\n",
    "FROM filter_tb\n",
    "INNER JOIN tasks \n",
    "ON filter_tb.CRAB_Workflow = tasks.TM_TASKNAME \n",
    "), \n",
    "finalize_tb AS (\n",
    "SELECT RecordTime, CMSPrimaryDataTier, WallClockHr, CoreHr, CpuTimeHr, ExitCode, CRAB_DataBlock, TM_IGNORE_LOCALITY, GlobalJobId, CommittedCoreHr, CommittedWallClockHr, \n",
    "       CASE \n",
    "           WHEN CRAB_DataBlock = 'MCFakeBlock' THEN 'PrivateMC'  \n",
    "           ELSE 'Analysis'\n",
    "       END AS CRAB_Type,        --- to differentiate between analysis and mc\n",
    "       'condor' AS type,        --- use to match specific data when use wildcard index pattern on grafana side\n",
    "       RecordTime AS timestamp  --- use `RecordTime` as timestamp\n",
    "FROM join_tb\n",
    ")\n",
    "SELECT * \n",
    "FROM finalize_tb \n",
    "\"\"\"\n",
    "tmpdf = spark.sql(query)\n",
    "tmpdf.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "            \"settings\": {\"index\": {\"number_of_shards\": \"1\", \"number_of_replicas\": \"1\"}},\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"RecordTime\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                    \"CMSPrimaryDataTier\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"GlobalJobId\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"WallClockHr\": {\"type\": \"long\"},\n",
    "                    \"CoreHr\": {\"type\": \"long\"},\n",
    "                    \"CpuTimeHr\": {\"type\": \"long\"},\n",
    "                    \"ExitCode\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"TM_IGNORE_LOCALITY\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"CRAB_Type\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"CRAB_DataBlock\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"CommittedCoreHr\": {\"type\": \"long\"}, \n",
    "                    \"CommittedWallClockHr\": {\"type\": \"long\"},\n",
    "                    \"type\": {\"ignore_above\": 2048, \"type\": \"keyword\"},\n",
    "                    \"timestamp\": {\"format\": \"epoch_millis\", \"type\": \"date\"},\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0506d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import osearch\n",
    "importlib.reload(osearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition rdd to make each partition small enough to load back to python kernel, serialize to dict, and send to os.\n",
    "# for 12M rows, number of from 27 days of data is 51, around 250k per partition.\n",
    "# try reducing partition to 20 once but make python kernel out-of-memory. \n",
    "# so, try to keep it around 200k per partition instead.\n",
    "partition_num = tmpdf.count() // 200000\n",
    "tmpdf = tmpdf.repartition(partition_num, 'RecordTime')\n",
    "total_part = tmpdf.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"Number of partition: {total_part}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f7a3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# send to os, serialize df one rdd partition at a time\n",
    "part = 0\n",
    "for docs in tmpdf.rdd.mapPartitions(lambda p: [[x.asDict() for x in p]]).toLocalIterator():\n",
    "    part += 1\n",
    "    print(f\"Partition: {part}/{total_part}, Length of partition: {len(docs)}\")\n",
    "    osearch.send_os_parallel(docs, index_name, schema, secretpath, yesterday_epoch, 20000) # batch_size is just arbitrary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "org.apache.spark:spark-avro_2.12:3.5.0"
    },
    {
     "name": "spark.executor.instances",
     "value": "20"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
